{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1F8oOHK5baLReodtx7a5P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Soumyaa2005/CyberSecurity-Assignment-2/blob/main/CS_Project2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GAP 1: Inadequate Cookie Analysis**"
      ],
      "metadata": {
        "id": "TlRNj6_b9RHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem:** The research paper highlights stateful tracking (like HTTP Cookies) is common, but conventional detection is a simple \"presence\" check. This lacks context on the cookie's security design.\n",
        "\n",
        "**Improvement:** Implement a **Cookie Hygiene Score (CHS)** to quantify the security of a cookie based on set attributes (`Secure`, `HttpOnly`, `SameSite`) and penalize for excessive lifespan.\n",
        "\n"
      ],
      "metadata": {
        "id": "-WP2OF9Z9cUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# analysis/cookie_hygiene.py\n",
        "\n",
        "import re, math, pandas as pd\n",
        "\n",
        "ATTRS = [\"secure\", \"httponly\", \"samesite\"]\n",
        "def parse_set_cookie(header):\n",
        "    # Parses a Set-Cookie header string into a dictionary of attributes\n",
        "    parts = [p.strip() for p in header.split(\";\")]\n",
        "    name, val = parts[0].split(\"=\", 1) if \"=\" in parts[0] else (parts[0], \"\")\n",
        "    flags = {k: False for k in ATTRS}\n",
        "    samesite = None; max_age=None; expires=None\n",
        "    for p in parts[1:]:\n",
        "        kv = p.split(\"=\", 1)\n",
        "        k = kv[0].strip().lower()\n",
        "        v = kv[1].strip().lower() if len(kv)==2 else True\n",
        "        if k in (\"secure\",\"httponly\"): flags[k]=True\n",
        "        elif k==\"samesite\": flags[\"samesite\"]=True; samesite=v\n",
        "        elif k==\"max-age\": max_age = int(v) if v.isdigit() else None\n",
        "        elif k==\"expires\": expires = v\n",
        "    return {\"name\":name,\"secure\":flags[\"secure\"],\"httponly\":flags[\"httponly\"],\n",
        "            \"samesite\":flags[\"samesite\"],\"samesite_val\":samesite,\n",
        "            \"max_age\":max_age,\"expires\":expires}\n"
      ],
      "metadata": {
        "id": "4Q9sqCop89Ae"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cookie_score(df_setcookie):  # df_setcookie columns: url, set_cookie_header\n",
        "    rows = []\n",
        "    for _, r in df_setcookie.iterrows():\n",
        "        meta = parse_set_cookie(r[\"set_cookie_header\"])\n",
        "        # Base score (max 3): +1 for Secure, +1 for HttpOnly, +1 for SameSite\n",
        "        score = (1 if meta[\"secure\"] else 0) + (1 if meta[\"httponly\"] else 0) + (1 if meta[\"samesite\"] else 0)\n",
        "        # Expiry Penalty: penalize cookies with a life > 30 days (potential tracking risk)\n",
        "        life_pen = 1 if (meta[\"max_age\"] and meta[\"max_age\"]>60*60*24*30) else 0\n",
        "        rows.append({**meta, \"url\": r[\"url\"], \"score\": score - life_pen})\n",
        "    out = pd.DataFrame(rows)\n",
        "    # Calculate Mean/Median Cookie Hygiene Score (CHS) per URL\n",
        "    chs = out.groupby(\"url\")[\"score\"].agg([\"mean\",\"median\",\"count\"]).reset_index().rename(\n",
        "        columns={\"mean\":\"chs_mean\",\"median\":\"chs_med\",\"count\":\"cookie_count\"})\n",
        "    return chs, out"
      ],
      "metadata": {
        "id": "gmN5jk-Y9kWM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GAP 2: Binary Navigator Fingerprinting Detection**\n"
      ],
      "metadata": {
        "id": "Zj9TXFHZ9qNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem:** Navigator fingerprinting is common, but the paper notes it only tracks **presence** and doesn't profile *which properties* are used. The basic model can't distinguish between a benign query and aggressive fingerprinting.\n",
        "\n",
        "\n",
        "**Improvement:** Extract **Navigator Specific Properties (NSP)** into a feature matrix, quantifying the breadth of properties accessed to identify aggressive fingerprinting attempts."
      ],
      "metadata": {
        "id": "LWUA6P5T92Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# analysis/extract_navigator.py\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "NAV_KEYS = [\"userAgent\",\"language\",\"languages\",\"platform\",\"deviceMemory\",\n",
        "            \"hardwareConcurrency\",\"plugins\",\"webdriver\",\"userAgentData.brands\",\n",
        "            \"userAgentData.platform\",\"userAgentData.mobile\"]\n",
        "\n",
        "def build_nsp(js_calls_df):  # columns: url, api, prop\n",
        "    # Filter for navigator API calls\n",
        "    df = js_calls_df[js_calls_df[\"api\"].str.contains(\"navigator\", case=False, na=False)].copy()\n",
        "    df[\"prop_norm\"] = df[\"prop\"].str.lower()\n",
        "\n",
        "    # Map raw properties to a standard set of NAV_KEYS\n",
        "    feats = {k.lower():k for k in NAV_KEYS}\n",
        "    df[\"prop_bucket\"] = df[\"prop_norm\"].map(lambda p: next((f for f in feats if f in p), None))\n",
        "\n",
        "    # Pivot to create a URL x Property matrix (counts of access)\n",
        "    mat = (df.dropna(subset=[\"prop_bucket\"])\n",
        "             .assign(val=1)\n",
        "             .pivot_table(index=\"url\", columns=\"prop_bucket\", values=\"val\", aggfunc=\"sum\", fill_value=0)\n",
        "             .reset_index())\n",
        "\n",
        "    # Convert to binary presence (1=used, 0=not used)\n",
        "    for c in [c for c in mat.columns if c!=\"url\"]:\n",
        "        mat[c] = (mat[c] > 0).astype(int)\n",
        "\n",
        "    # Total count of unique navigator keys accessed\n",
        "    mat[\"n_nav_keys\"] = mat.drop(columns=[\"url\"]).sum(axis=1)\n",
        "    return mat"
      ],
      "metadata": {
        "id": "YyfMS8ZE9wmC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GAP 3: Lack of Predictive Context (Machine Learning)**"
      ],
      "metadata": {
        "id": "Tt4fdqKdBtlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem:** Analyzing tracking mechanism presence alone is an **unreliable test** for classifying a webpage. Security solutions need context to differentiate malicious from benign.\n",
        "\n",
        "**Improvement:** Integrate the extracted quantitative features (CHS, NSP) into a simple machine learning model (Logistic Regression) to establish a baseline for **predictive context**."
      ],
      "metadata": {
        "id": "8lgKzCIgBzRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# analysis/build_features.py\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "\n",
        "def train_eval(X, y):\n",
        "    # Standard train/test split for evaluation\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "    # Train Logistic Regression classifier\n",
        "    clf = LogisticRegression(max_iter=200).fit(Xtr, ytr)\n",
        "    proba = clf.predict_proba(Xte)[:,1]\n",
        "\n",
        "    # Return evaluation metrics and feature coefficients (risk weights)\n",
        "    return {\n",
        "        \"AUC\": roc_auc_score(yte, proba),\n",
        "        \"AP\": average_precision_score(yte, proba),\n",
        "        \"coef\": dict(zip(X.columns, clf.coef_[0]))\n",
        "    }\n",
        "\n",
        "# Placeholder for the data loading and model execution part\n",
        "# crawl (baseline vs EASP)"
      ],
      "metadata": {
        "id": "iJb8S5ysCKFj"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}